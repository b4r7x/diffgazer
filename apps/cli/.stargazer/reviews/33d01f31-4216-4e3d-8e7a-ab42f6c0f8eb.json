{
  "metadata": {
    "id": "33d01f31-4216-4e3d-8e7a-ab42f6c0f8eb",
    "projectPath": "/Users/voitz/Projects/stargazer/apps/cli",
    "createdAt": "2026-01-22T19:04:20.470Z",
    "staged": false,
    "branch": "feature/review-bounding",
    "overallScore": null,
    "issueCount": 0,
    "criticalCount": 0,
    "warningCount": 0,
    "sessionId": null
  },
  "result": {
    "summary": "```json\n{\n  \"summary\": \"This extensive diff introduces a new chat feature, allows linking reviews to chat sessions, adds configuration for AI `maxTokens`, and significantly improves the robustness and user experience of the AI review process. Key improvements include better handling of AI response parsing, progress display for reviews, and a detailed plan for storage integration tests. The changes are largely positive, enhancing functionality, reliability, and maintainability.\",\n  \"issues\": [\n    {\n      \"severity\": \"suggestion\",\n      \"category\": \"logic\",\n      \"file\": \"apps/cli/src/app/app.tsx\",\n      \"line\": 56,\n      \"title\": \"Unclear Error Handling for Review Linking\",\n      \"description\": \"In `handleDiscussReview`, if the `PATCH /reviews/{review.metadata.id}` API call fails, the error is caught and ignored. This means a discussion session might be created for a review, but the review itself might not be successfully linked to the session, leading to an inconsistent state without user feedback.\",\n      \"suggestion\": \"Instead of ignoring the error, consider adding a user-facing warning or a retry mechanism. For example, `console.error` and then `toast.error('Failed to link review to session.')` (if a toast system exists). This would inform the user about the partial failure.\"\n    },\n    {\n      \"severity\": \"suggestion\",\n      \"category\": \"security\",\n      \"file\": \"apps/server/src/services/review.ts\",\n      \"line\": 64,\n      \"title\": \"Prompt Injection Defense Weakened by Markdown Fence\",\n      \"description\": \"The prompt structure for AI review has been changed from `<code-diff>...</code-diff>` XML tags to markdown code fences (` ```diff\\\\n{diff}\\\\n ````). While markdown fences are generally understood by LLMs, XML tags provide a stronger, more explicit semantic boundary for content that should be treated as data rather than instructions. An attacker could potentially craft diff content that 'breaks out' of the markdown fence more easily than XML tags, leading to prompt injection vulnerabilities. The `sanitizeUnicode` function is good, but the overall structural robustness is slightly reduced.\",\n      \"suggestion\": \"Monitor for any potential prompt injection attempts with the new markdown fence approach. If issues arise, consider alternative, more robust data encapsulation methods (e.g., base64 encoding the diff) or re-evaluate the use of XML-like tags if the AI model can be reliably constrained to them. The current `sanitizeUnicode` and explicit prompt instructions are good mitigations, but the change itself is a slight step back in strictness.\"\n    },\n    {\n      \"severity\": \"suggestion\",\n      \"category\": \"performance\",\n      \"file\": \"apps/server/src/services/review.ts\",\n      \"line\": 28,\n      \"title\": \"Increased `MAX_DIFF_SIZE_BYTES`\",\n      \"description\": \"The `MAX_DIFF_SIZE_BYTES` has been increased from 100KB to 512KB. While this allows for reviewing larger diffs, processing very large inputs can lead to higher latency, increased token consumption (and cost), and potential memory issues on the server or AI provider side. Although the `maxTokens` setting is now configurable for the AI model, the input diff size itself remains a factor.\",\n      \"suggestion\": \"Ensure that the AI client and server infrastructure can comfortably handle diffs up to 512KB. Consider adding more granular warnings or even breaking down very large diffs into smaller chunks on the client-side before sending, if the `chunked` review endpoint doesn't fully address this for *all* scenarios. The new `chunked` review mechanism helps mitigate this by processing files individually, but it's still worth noting the overall limit increase for the non-chunked endpoint.\"\n    },\n    {\n      \"severity\": \"suggestion\",\n      \"category\": \"style\",\n      \"file\": \"apps/cli/src/app/app.tsx\",\n      \"line\": 109,\n      \"title\": \"Centralize Keyboard Shortcut Management\",\n      \"description\": \"The `App` component now contains a growing number of hardcoded keyboard shortcuts within `useInput` callbacks (e.g., 'g', 'd', 'r', 'c', 'h', 'H', 'S', 'q', 'b', 'escape'). As the application expands, this approach can become difficult to manage, prone to conflicts, and less testable. The logic for which keys are active depends on the current `view` state, which is handled, but the enumeration of keys is spread out.\",\n      \"suggestion\": \"Consider implementing a more centralized keyboard shortcut manager or a declarative mapping of views to available shortcuts. This could involve a context provider, a custom hook, or a configuration object that maps key presses to actions, making it easier to define, modify, and prevent conflicts as new features are added. This would improve maintainability and potentially testability.\"\n    },\n    {\n      \"severity\": \"suggestion\",\n      \"category\": \"documentation\",\n      \"file\": \"packages/core/src/ai/providers/gemini.ts\",\n      \"line\": 78,\n      \"title\": \"Clarify `DEFAULT_MAX_OUTPUT_TOKENS` vs `config.maxTokens`\",\n      \"description\": \"The `DEFAULT_MAX_OUTPUT_TOKENS` is increased to 65536. The `getGenerationConfig` function uses `config.maxTokens ?? DEFAULT_MAX_OUTPUT_TOKENS`. It's good that `config.maxTokens` takes precedence, allowing user override. However, the previous `DEFAULT_MAX_TOKENS = 4096` was a more conservative default. While 65536 might be the model's maximum, setting such a high default could lead to higher costs or longer response times for users who don't explicitly configure it.\",\n      \"suggestion\": \"Add a comment explaining the rationale behind choosing 65536 as the default for `DEFAULT_MAX_OUTPUT_TOKENS` if it's the model's actual maximum. Also, consider if a more moderate default (e.g., 8192 or 16384) might be more appropriate for average use cases to balance cost/performance, while still allowing users to go up to 65536 via configuration. The current setup is technically correct given the new config option, but the default might impact unconfigured users.\"\n    }\n  ],\n  \"overallScore\": 9\n}\n```",
    "issues": []
  },
  "gitContext": {
    "branch": "feature/review-bounding",
    "fileCount": 33
  }
}
